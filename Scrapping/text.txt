from bs4 import BeautifulSoup

# contoh parse HTML lokal atau dari Selenium
with open('htmlnya.html', 'r', encoding='utf-8') as file:
    soup = BeautifulSoup(file, 'html.parser')

# Ambil semua baris di dalam tabel
rows = soup.select('div.bzg_c table tr')

# Siapkan variabel untuk menyimpan data
nama = None
jenis_usaha = None
website = None
email = None
telepon = None
sosmed_links = []

for row in rows:
    label = row.select_one('.td-name')
    value = row.select_one('.td-content')

    if not label or not value:
        continue

    key = label.get_text(strip=True)
    val = value.get_text(strip=True)

    # Cek dan simpan yang dibutuhkan saja
    if key == "Nama":
        nama = val

    elif key in ["Bidang Usaha Utama", "Sektor"]:  # dua kemungkinan
        jenis_usaha = val

    elif key == "Situs":
        if value.a:
            website = value.a.get_text(strip=True)

    elif key == "Alamat Email":
        email = val

    elif key == "Telepon":
        telepon = val

# Cari sosial media (kalau ada link-link di halaman)
for a in soup.find_all('a', href=True):
    href = a['href']
    if "instagram.com" in href or "facebook.com" in href or "linkedin.com" in href:
        sosmed_links.append(href)

# Print hasilnya
print("Nama:", nama)
print("Jenis Usaha:", jenis_usaha)
print("Website:", website)
print("Email:", email)
print("Telepon:", telepon)
print("Sosial Media:", sosmed_links if sosmed_links else "Tidak ditemukan")

///////////
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
import time

# Setup Chrome Driver
opsi = webdriver.ChromeOptions()
servis = Service('chromedriver.exe')
driver = webdriver.Chrome(service=servis, options=opsi)

# Masuk ke halaman daftar perusahaan
driver.get("https://www.idx.co.id/id/perusahaan-tercatat/daftar-perusahaan-tercatat/")
time.sleep(5)  # tunggu loading

# Ambil halaman
soup = BeautifulSoup(driver.page_source, "html.parser")

# Cari semua link ke profil perusahaan
base_url = "https://www.idx.co.id"
link_perusahaan = []

for link in soup.find_all("a", href=True):
    href = link['href']
    if "/id/perusahaan-tercatat/profil-perusahaan-tercatat/" in href:
        full_url = base_url + href
        link_perusahaan.append(full_url)

# Tampilkan hasil
for url in link_perusahaan:
    print(url)

driver.quit()

base_url = "https://www.idx.co.id/id/perusahaan-tercatat/profil-perusahaan-tercatat?page={}"

companies = []

for page in range(1, 5):  # Ganti range sesuai jumlah halaman
    url = base_url.format(page)
    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')

    # Cari selector perusahaan. Contoh ini perlu disesuaikan dengan struktur HTML aktual
    rows = soup.select('table tbody tr')
    for row in rows:
        cols = row.find_all('td')
        if len(cols) > 4:  # Pastikan jumlah kolom sesuai
            name = cols[1].text.strip()
            sector = cols[2].text.strip()
            website = cols[3].text.strip()
            contact = cols[4].text.strip()
            # Sosmed biasanya tidak tercantum langsung, bisa cek di website perusahaan
            companies.append({
                'Nama': name,
                'Sektor': sector,
                'Website': website,
                'Kontak': contact,
                'Sosmed': ''  # Kosongkan, isi manual/crawling lanjut ke website perusahaan
            })

# Simpan ke CSV atau Excel
df = pd.DataFrame(companies)
df.to_csv('daftar_perusahaan_idx.csv', index=False)



/////////////////////
from flask import Flask, render_template, request, redirect, send_file
import pandas as pd
import io

app = Flask(__name__)
data_perusahaan = []

@app.route("/", methods=["GET", "POST"])
def index():
    global data_perusahaan
    if request.method == "POST":
        nama = request.form["nama"]
        sektor = request.form["sektor"]
        website = request.form["website"]
        sosmed = request.form["sosmed"]
        kontak = request.form["kontak"]

        data_perusahaan.append({
            "Nama": nama,
            "Sektor": sektor,
            "Website": website,
            "Sosmed": sosmed,
            "Kontak": kontak
        })
        return redirect("/")

    return render_template("index.html", data=data_perusahaan)

@app.route("/download")
def download_excel():
    global data_perusahaan
    df = pd.DataFrame(data_perusahaan)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Perusahaan')
    output.seek(0)
    return send_file(output, mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                     download_name="data_perusahaan.xlsx", as_attachment=True)

if __name__ == "__main__":
    app.run(debug=True)